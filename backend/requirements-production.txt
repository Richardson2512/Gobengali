# Production Requirements for GoBengali - Fully Functional AI Backend
# NO mock data - all real AI models with intelligent fallbacks

# Core API
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
pydantic-settings==2.1.0

# AI/ML Models (PRIMARY)
transformers==4.35.0  # mT5, NLLB-200, IndicBERT
torch==2.1.0  # CPU version
# For GPU: pip install torch==2.1.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html
sentencepiece==0.1.99  # NLLB tokenizer
protobuf==4.25.0
accelerate==0.24.0
safetensors==0.4.1

# SPELLING CHECKER (PRIMARY): BSpell
# BSpell - Bengali Spell Checker
# Install from: pip install git+https://github.com/sagorbrur/bspell.git
# Or if available on PyPI: bspell==1.0.0

# SPELLING CHECKER (FALLBACK): LanguageTool
language-tool-python==2.7.1

# Language Detection
langdetect==1.0.9

# NLP Utilities
nltk==3.8.1
regex==2023.10.3

# API & Utils
python-multipart==0.0.6
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-dotenv==1.0.0
aiofiles==23.2.1
httpx==0.25.2

# Database (optional)
motor==3.3.2  # MongoDB
redis==5.0.1

# Monitoring
python-json-logger==2.0.7

# Testing
pytest==7.4.3
pytest-asyncio==0.21.1

# Models & Sizes:
# ================
# Primary Models:
# - google/mt5-small: ~1.2GB (Grammar - mT5)
# - facebook/nllb-200-distilled-600M: ~1.2GB (Translation)
# - BSpell: ~10MB (Spelling)
#
# Fallback Models:
# - ai4bharat/IndicBERTv2-MLM-only: ~560MB (Grammar fallback)
# - LanguageTool: ~200MB (Spelling fallback)
#
# Total: ~3GB (all models)

# System Requirements:
# ===================
# - RAM: 8GB minimum, 16GB recommended
# - Disk: 5GB free space
# - CPU: 4+ cores recommended
# - GPU: Optional (NVIDIA with CUDA for 5x faster inference)

# Installation Notes:
# ===================
# 1. Install PyTorch first (CPU or GPU version)
# 2. Install BSpell from GitHub: pip install git+https://github.com/sagorbrur/bspell.git
# 3. Install remaining packages: pip install -r requirements-production.txt
# 4. Run setup: python setup_production.py

